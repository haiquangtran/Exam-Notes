# Developing Windows Azure and Web Services

# Accessing Data
- **ADO.NET**
  - Designed to support large loads and to excel at security, scalability, flexibility, and dependability.
  - Has a bias toward a disconnect model. For example, when using individual commands such as INSERT, UPDATE, or DELETE Statements, you simply open a connection to the database, execute the command, then close the connection as quickly as possible etc. On the query side, use a select and close the connection, using only the local version then push changes back to DB.  
  - Take advantage of the benefits of ADO.NET by minimizing unnecessary connections to the database. If you open it, you should close it. Close() should be in the finally block of a try/catch/finally. 
  - **Connected vs Disconnected Model**
    - Connections are expensive for a RDBMS to maintain. (Consumes processing and networking resources, and DB's can only maintain finite number of connections at once). Also connections can hold locks on data, causing concurrency problems. Keeping connections closed and opening them only for short periods will help mitigate many of database-focused performance problems. 
    - To improve efficiency, ADO.NET uses connection pooling. Since ADO.NET opens and closes connections at a high rate, the minor overheads in establishing connection and cleaning up a connection begin to affect performance. Connection pooling helps combat this problem.
    - Connection Pooling
      - Creates a few connections (let's say 50). 
      - Opens them up, negotiates with the RDBMS about how it will communicate with it, then enables requests to share these active connections, 50 at a time. 
      - So instead of taking up valuable resources performing the same non-trivial task 10,000 times, it does it only 50 times and then efficiently funnels all 10,000 requests through these 50 channels.
      - This means each of these 50 connections would have to handle 200 requests in order to process all 10,000 requests within that minute.
      - Manages the number of active connections for you. You can specify the max number of connections in a connection string. 
      	- With ADO.NET 4.5 accessing SQL server 2012, this limit defaults to 100 simultaneous connections and can scale anywhere between that and 0 without you as a developer having to think about it.
  - **Why choose ADO.NET?**
    - Consistency, ADO.NET as a data access technology has been around longer than other options available. Unless it's a relatively new application or an older application that has been updated to use one of the newer alternatives, ADO.NET is already being used to interact with the database.
    - Stability, both in terms of evolution and quality of technology. ADO.NET is firmly established and is unlikely to change in any way other than feature additions. 
    - ADO.NET is an easy library to learn and understand. It's been around for ages, there are providers for almost every well-known database, and many lesser-known database vendors ahve providers available for ADO.NET.
    - Can use ADO.NET against Windows Azure's SQL databases with essentially no difference in coding. 
- **.NET Framework data providers**
  - .NET Framework data providers are described as "components that have been explicitly designed for data manipulation and fast, forward-only, read-only access to data."
  - **.NET Framework data provider overview**
    - DbConnection = Necessary for nay DB interaction. Care should be taken to close connections as soon as possible after using them.
    - DbCommand = Necessary for all DB interactions in addition to Connection. Parameterization should be done only through the Parameters collection (You should always do parameterization for security reasons). Concatenated strings should never be used for the body of the query or as alternatives to parameters.
    -  DbDataReader = Ideally suited to scenarios in which speed is the most critical aspect because of its forward-only nature, similar to a Stream. This provides read-only access to the data.
    - DbDataAdapter = Used in conjunction with a Connection and Command object to populate a DataSet or an individual DataTable, and can also be used to make modifications back to the database. Changes can be batched so that updates avoid unnecessary roundtrips to the database.
    - DataSet = In-memory copy of the RDBMS or portion of RDBMS. Collection of DataTable objects with their relationships, metadata and commands to interact with it. 
    - DataTable = Corresponds to specific view of data, whether from a SELECT query or generated from .NET code. Similar to RDBMS tables but it is partially populated. It tracks state of data stored in it so, when data is modified, you can tell which records need to be saved back into the database.
- **DataSet vs DataReader**
  - When querying data, you can use one these two mechanisms: DataAdapter (which uses DataSet) or DataReader.
  - Every SELECT query operation you employ in ADO.NET uses a DataReader. A DataAdapter uses a DataReader to populate the returned DataSet or DataTable.
  - Using a DataReader produces faster results than using a DataAdapter to return the same data. 
  - DataReaders provide multiple asynchronous methods taht can be employed. DataAdapters on the other hand, essentially have only synchronous methods. With small-sized record sets, differences in performance of using asynch methods are trivial. On large queries that take time, a DataReader, with asynch methods can greatly enhance the UX. 
  - The Fill method of DataAdapter only lets you populate DataSets and DataTables. (Writing for custom business objects may impact on app responsiveness and memory because of this)
  - The Fill method of DataAdapter comples onyl when all data has been retrieved and added to the DataSet or DataTable. This enables you to immediately determinet he number of records in any given table. However, a DataReader can indicate whether data was returned (via HasRows property), but the only way to know the exact record count returned is to iterate through and count it specifically.
  - Can iterate through DataReader only once and only in a forward-only fashion. You can iterate through a DataTable any number of times in any manner you see fit.
  - Both enable you to execute multiple queries and retrieve multiple return sets, but only the DataSet lets you closely mimic the behaviour of a relational DB (i.e. relationships between tables, or enforce constraints on properties etc).
  -  DataSets can be loaded directly from XML and can be persisted to XML natively. They are consequently inhernetly serializable, which affords many features not natively available to DataReaders. Can also easily pass DataSets or DataTables between tiers becuase it is already serializable, but you can't do the same with DataReader. 
  - DataSet is also an expensive object with a large memory footprint. It is generally ill-advised to store it in a Session or Viewstate variables, or pass it across multiple application tiers because fo the expensive nature of the object. If you serialize a DataSet, process with caution!
  - Using any of the asynchronous methods available with the SqlDataReader, you can provide feedback (although somewhat limited) to the client application.
- **Entity Framework**
  - Entity Framework = allows developers to focus on application code, not the underlying plumbing code necessary to communicate with a DB efficiently and securely.
  - LINQ-to-SQL was on eof the first Microsoft initiatives to build an ORM tool. 
  - The primary benefit is that it enables developers to manipulate data as domain-specific objects without regard to the underlying structure of the data store. 
  - From a developer's point of view, Entity Framework enables developers to work with entities (such as Customers, Accounts etc), known as the conceptual model. Entity Framework is responsible for mapping these entities and their corresponding properties to the underlying data source. 
  - **Entity Framework Modeling**
  - The 3 parts of the EF model:
    - **The Conceptual Model**
      - Handled via the conceptual schema defintion language (CSDL). In older versions of EF, it existed in a file with a .csdl extension. 
    - **Data Storage**
     - The data storage aspect is handled through the store schema definition language (SSDL). In older versions of EF, it existed in a file with a .ssdl file extension. 
    - **Mapping specification language**
      - The mapping between the CSDL and SSDL is handled via the mapping specification language (MSL). In older versions of EF, it existed in a file with an .msl file extension. However, all 3 are in a single file, it is important to understand differences between the three. 
    - The back-end components can be completely changed without affecting the conceptual model by allowing the changes to be absorbed by the MSL's mapping logic.
    - Compare this with ADO.NET. If you took any of the samples provided and have to change them to use an Oracle DB, there would be major changes necessary to all the code written. In the EF, you'd simply focus on the business objects and let the storage model and mappings handle the change to how the data came from and got back to the DB. 
    - **Creating a new Entity Framework Project**
      - **Entity Model Designer**
        - Creates the .edmx file and enables manipulation of almost every aspect of the model (create/update/delete entities), manipulate associations and mappings, modify inheritance relationships etc.
      - **Entity Data Model Wizard** 
        - It is the true starting point of building your conceptual model. Enables you to use an existing data store instance.
      - **The Create Database Wizard**
        - Enables you to do exact opposite of the previous item. Instead of starting with a database, it enables you to fully  build and manipulate your conceptual model, and it takes care of building the actual DB based on the conceptual model. 
      - **The Update Model Wizard**
        - After your model is built, it enables you to fully modify every aspect of the conceptual model. It can let you do the same for both the storage model and the mappings that are defined between them.
    - **T4 Code Generation**
      - T4 text template files can be often identified by the .tt extension.
      - T4 is the templating and code generation engine that Entity Framework uses to generate code so you don't have to manage it yourself. 
      - These are generated based from the conceptual model in the .edmx file.
  - **Entity Data Model Designer**
    - When you have EDMX Designer open, View -> Other Windows -> Entity Data Model Browser. This is useful as your .edmx begins to cover a large number of tables and you have difficulty locating a particular one. 
    - Inheritance and Complex type options can be done in the .edmx file. 
    - **Table per Hierarchy (TPH)**
      - Is the default mapping strategy.
      - It creates a single table for all objects in an inheritance hierarchy, and it simply has nullable columns for fields that aren't common across all types. It also adds a Disciminator column so EF can keep track of the type of each individual record.  - This strategy is often the best balance of tradeoffs available because it provides the best performance. 
        - The primary disadvantage is that your data is slightly denormalized.
    - **Table per Type (TPT)**
      - Alternative strategy
      - Creates a table for your base type that has all common fields in it, a table for each child type that stores the additional fields, as well as an ID to the base type's record that stores the common fields. The multiple inheriting types' tables are linked to one another via a foreign key that has a shared primary key value. In this case, EF has to perform a join across multiple tables. 
        - The primary disadvantage is that your performance suffers.
    - **Table per Concrete Type (TPC) and Mixed Inheritance**
      - These are the other options. These are not usually the most practical choice because of the limited tooling support for them.
      - **Currently not supported in the EDM Designer, although the EF runtime does support them**
    - **Complex Type**
      - A complex type is a logical designation for a common group of fields on multiple entities. Used for reusability, as it allows you to use repeated groups of fields (e.g. a date range). 
    - **Stored Procedure Mapping**
      - based on the STored Procedures defined in the database. It enables you to specify an Update, and Delete function. SElect queries are already handled by the langauge semantics of LINQ.
  - **ObjectContext vs DbContext** 
    - Olders versions of EF did not have DbContext but had ObjectContext.
    - Modern versions still support the ObjectContext object, and you can even consume modern EF in much the same way as the older versions. 
    - If you want to work in an ObjectContext scenario, you have 3 primary options:
      - Legacy approach
        - Follow similar steps using VS 2008 or 2010 to create an .edmx file and its corresponding ObjectContext and entities
      - Downgrading your entities
        - Take the .edmx file that was generated, open the properties window, and set focus in the Design'ers canvas. 
        - Change the Code Generation Strategy from None to default in the Properties window.
        - Delete the two .tt files listed as children to your .edmx file in the Solution explorer window
      - Hybrid Approach
        - Get the ObjectContext (via a nonobvious approach) from the DbContext and work with the ObjectContext directly. Note that even with modern versions of EF, in some rare and advanced scenarios this is still required. 
  - **ObjectContext management**
    - Two things happen when ObjectContext constructor is called:
      - The generated context inherits several items from the ObjectContext base class, including a property known as ContextOptions and an event named OnContextCreated.
      - The ContextOptions class has five primary properties you can manipulate:
        - ** LazyLoadingEnabled (VERY IMPORTANT) **
          - If left unspecified, the default is true
          - Lazy loading enables entities to be loaded on-demand without thought by the developer. Although this can be handy, this behaviour can have very serious performance implications depending on how relationships are set up. 
          - Feature development and even app architecture might change one way or another based on the use of this feature. 
          - ** In EF, lazy loading is triggered based on a Navigation Property being accessed. By simply referencing a navigation property, that entity is then loaded **
            - Accessing a navigation property causes another roundtrip to the database to fetch that data. 
            - If you loop through a collection of entities, an individual roundtrip is made to the database for each entity. 
          - If you have LazyLoadedEnabled to false, you have two options:
            - Can either use explicit lazy loading with the ObjectContext's LoadProperty() method or you can use eager loading with the ObjectSet's Include() method.  
            - With eager loading, you must specify up front what related data you want loaded.
            - Explicit lazy loading, like regular lazy loading, can reduce the amount of data flowing back and forth, it's a chatty pattern, whereas eager loading is a chunky pattern. 
            - **Chatty vs Chunky Pattern**
              - Chatty is taking into consideration the latency or delay of making a roundtrip. It is doing parts at a time to get to the whole. i.e. Making multiple calls to the DB to sum the result or something.
              - Chunky pattern is doing it all at once type. i.e. Making 1 call to the database. 
              - The best option depends on different factors such as the transfer time and the delay with the roundtrips. Typically favour CHUNKY over chatty!
        - ** ProxyCreationEnabled **
          - Determines whether proxy objects should be created for custom data classes that are persistence-ignorant, such as a plan old common object (POCO) entities. 
          - Default is set to true. (Generally isn't something you need to be concerned about if you set to false).
          - Only available in .NET Framework 4.5.
        - ** UseConsistentNullReferenceBehaviour **
          - When set to false, you can attempt to set navigation property to null and nothing will happen when you try and save it. 
          - When set to true, Allows you to set navigation property to null (which may throw error if the field is not nullable)
          - LOOK THIS UP. 
        - ** UseCSharpNullComparisonBehaviour **
          - Main implication of this property is that it changes queries that require null comparisons.
        - ** UseLegacyPreserverChangesBehaviour **
  - **ObjectContext entities**
    - Each entity you define int he conceptual model gets a class generated.
    - There are few options for the types of entities that can be generated.
      - In EF 4, the default base class for entities was EntityObject.
      - Two other types of entities could also be created: POCO entities (no required base class for these) and Self-Tracking entities (STEs) (no base class, but they implemented IObjectWithChangeTracker and INotifyPropertyChanged). 
    - Attributes (In the generated classes): 
      - **EdmEntityTypeAttribute**
        - Contains the item's namespace and the Name.
        - Does little more than tell the world that the class is an EDM type and then provides a basic amount of supporting information about it. 
      - **SerializableAttribute**
        - The entity class needs to be serializable, if you want to persist the object locally, pass it between tiers, or do much else with it. 
      - **DataContractAttribute**
        - Must decorate the class (required for WCF's default serialization)
      - Look at the definition of each class that's generated corresponding to each entity and as much as possible in the .edmx. 
        - Each property needs to be marked with the DataMember attribute
        - The EdmScalarProperty attribute decorates each property definition
        - The two main properties that it definares are EntityKeyProperty and IsNullable.
  - **Why choose the Entity Framework (EF)**
    - Microsoft has made a tremendous investment in the EF and continues to, which is indicative of its future plans and where it intends data access to go. 
    - The toolset has matured, the following are the benefits:
      - There is a tremendous tooling support that enables you to build and maintain the data access elements of your application in a much shorter time than ADO.NET would
      - You can focus on dealing with business objects of your application and don't have to spend your time voerly concerned about the underlying data store. 
      - It allows for a clear separation of concerns between the conceptual model and the underlying data store. 
      - The data store can be changed (quite dramatically) without having to rewrite core application logic. Microsoft has made a tremendous investment into the EF, and all indiciations point to the fact it has made a firm commitment to the success of the EF.
      - Built to fit right in with WCF, which makes them complementary technology.
- **WCF Data Services**
  
