# Developing Windows Azure and Web Services

# Accessing Data
- **ADO.NET**
  - Designed to support large loads and to excel at security, scalability, flexibility, and dependability.
  - Has a bias toward a disconnect model. For example, when using individual commands such as INSERT, UPDATE, or DELETE Statements, you simply open a connection to the database, execute the command, then close the connection as quickly as possible etc. On the query side, use a select and close the connection, using only the local version then push changes back to DB.  
  - Take advantage of the benefits of ADO.NET by minimizing unnecessary connections to the database. If you open it, you should close it. Close() should be in the finally block of a try/catch/finally. 
  - **Connected vs Disconnected Model**
    - Connections are expensive for a RDBMS to maintain. (Consumes processing and networking resources, and DB's can only maintain finite number of connections at once). Also connections can hold locks on data, causing concurrency problems. Keeping connections closed and opening them only for short periods will help mitigate many of database-focused performance problems. 
    - To improve efficiency, ADO.NET uses connection pooling. Since ADO.NET opens and closes connections at a high rate, the minor overheads in establishing connection and cleaning up a connection begin to affect performance. Connection pooling helps combat this problem.
    - Connection Pooling
      - Creates a few connections (let's say 50). 
      - Opens them up, negotiates with the RDBMS about how it will communicate with it, then enables requests to share these active connections, 50 at a time. 
      - So instead of taking up valuable resources performing the same non-trivial task 10,000 times, it does it only 50 times and then efficiently funnels all 10,000 requests through these 50 channels.
      - This means each of these 50 connections would have to handle 200 requests in order to process all 10,000 requests within that minute.
      - Manages the number of active connections for you. You can specify the max number of connections in a connection string. 
      	- With ADO.NET 4.5 accessing SQL server 2012, this limit defaults to 100 simultaneous connections and can scale anywhere between that and 0 without you as a developer having to think about it.
  - **Why choose ADO.NET?**
    - Consistency, ADO.NET as a data access technology has been around longer than other options available. Unless it's a relatively new application or an older application that has been updated to use one of the newer alternatives, ADO.NET is already being used to interact with the database.
    - Stability, both in terms of evolution and quality of technology. ADO.NET is firmly established and is unlikely to change in any way other than feature additions. 
    - ADO.NET is an easy library to learn and understand. It's been around for ages, there are providers for almost every well-known database, and many lesser-known database vendors ahve providers available for ADO.NET.
    - Can use ADO.NET against Windows Azure's SQL databases with essentially no difference in coding. 
- **.NET Framework data providers**
  - .NET Framework data providers are described as "components that have been explicitly designed for data manipulation and fast, forward-only, read-only access to data."
  - **.NET Framework data provider overview**
    - DbConnection = Necessary for nay DB interaction. Care should be taken to close connections as soon as possible after using them.
    - DbCommand = Necessary for all DB interactions in addition to Connection. Parameterization should be done only through the Parameters collection (You should always do parameterization for security reasons). Concatenated strings should never be used for the body of the query or as alternatives to parameters.
    -  DbDataReader = Ideally suited to scenarios in which speed is the most critical aspect because of its forward-only nature, similar to a Stream. This provides read-only access to the data.
    - DbDataAdapter = Used in conjunction with a Connection and Command object to populate a DataSet or an individual DataTable, and can also be used to make modifications back to the database. Changes can be batched so that updates avoid unnecessary roundtrips to the database.
    - DataSet = In-memory copy of the RDBMS or portion of RDBMS. Collection of DataTable objects with their relationships, metadata and commands to interact with it. 
    - DataTable = Corresponds to specific view of data, whether from a SELECT query or generated from .NET code. Similar to RDBMS tables but it is partially populated. It tracks state of data stored in it so, when data is modified, you can tell which records need to be saved back into the database.
- **DataSet vs DataReader**
  - When querying data, you can use one these two mechanisms: DataAdapter (which uses DataSet) or DataReader.
  - Every SELECT query operation you employ in ADO.NET uses a DataReader. A DataAdapter uses a DataReader to populate the returned DataSet or DataTable.
  - Using a DataReader produces faster results than using a DataAdapter to return the same data. 
  - DataReaders provide multiple asynchronous methods taht can be employed. DataAdapters on the other hand, essentially have only synchronous methods. With small-sized record sets, differences in performance of using asynch methods are trivial. On large queries that take time, a DataReader, with asynch methods can greatly enhance the UX. 
  - The Fill method of DataAdapter only lets you populate DataSets and DataTables. (Writing for custom business objects may impact on app responsiveness and memory because of this)
  - The Fill method of DataAdapter comples onyl when all data has been retrieved and added to the DataSet or DataTable. This enables you to immediately determinet he number of records in any given table. However, a DataReader can indicate whether data was returned (via HasRows property), but the only way to know the exact record count returned is to iterate through and count it specifically.
  - Can iterate through DataReader only once and only in a forward-only fashion. You can iterate through a DataTable any number of times in any manner you see fit.
  - Both enable you to execute multiple queries and retrieve multiple return sets, but only the DataSet lets you closely mimic the behaviour of a relational DB (i.e. relationships between tables, or enforce constraints on properties etc).
  -  DataSets can be loaded directly from XML and can be persisted to XML natively. They are consequently inhernetly serializable, which affords many features not natively available to DataReaders. Can also easily pass DataSets or DataTables between tiers becuase it is already serializable, but you can't do the same with DataReader. 
  - DataSet is also an expensive object with a large memory footprint. It is generally ill-advised to store it in a Session or Viewstate variables, or pass it across multiple application tiers because fo the expensive nature of the object. If you serialize a DataSet, process with caution!
  - Using any of the asynchronous methods available with the SqlDataReader, you can provide feedback (although somewhat limited) to the client application.
- **Entity Framework**
  - Entity Framework = allows developers to focus on application code, not the underlying plumbing code necessary to communicate with a DB efficiently and securely.
  - LINQ-to-SQL was on eof the first Microsoft initiatives to build an ORM tool. 
  - The primary benefit is that it enables developers to manipulate data as domain-specific objects without regard to the underlying structure of the data store. 
  - From a developer's point of view, Entity Framework enables developers to work with entities (such as Customers, Accounts etc), known as the conceptual model. Entity Framework is responsible for mapping these entities and their corresponding properties to the underlying data source. 
  - **Entity Framework Modeling**
  - The 3 parts of the EF model:
    - **The Conceptual Model**
      - Handled via the conceptual schema defintion language (CSDL). In older versions of EF, it existed in a file with a .csdl extension. 
    - **Data Storage**
     - The data storage aspect is handled through the store schema definition language (SSDL). In older versions of EF, it existed in a file with a .ssdl file extension. 
    - **Mapping specification language**
      - The mapping between the CSDL and SSDL is handled via the mapping specification language (MSL). In older versions of EF, it existed in a file with an .msl file extension. However, all 3 are in a single file, it is important to understand differences between the three. 
    - The back-end components can be completely changed without affecting the conceptual model by allowing the changes to be absorbed by the MSL's mapping logic.
    - Compare this with ADO.NET. If you took any of the samples provided and have to change them to use an Oracle DB, there would be major changes necessary to all the code written. In the EF, you'd simply focus on the business objects and let the storage model and mappings handle the change to how the data came from and got back to the DB. 
    - **Creating a new Entity Framework Project**
      - **Entity Model Designer**
        - Creates the .edmx file and enables manipulation of almost every aspect of the model (create/update/delete entities), manipulate associations and mappings, modify inheritance relationships etc.
      - **Entity Data Model Wizard** 
        - It is the true starting point of building your conceptual model. Enables you to use an existing data store instance.
      - **The Create Database Wizard**
        - Enables you to do exact opposite of the previous item. Instead of starting with a database, it enables you to fully  build and manipulate your conceptual model, and it takes care of building the actual DB based on the conceptual model. 
      - **The Update Model Wizard**
        - After your model is built, it enables you to fully modify every aspect of the conceptual model. It can let you do the same for both the storage model and the mappings that are defined between them.
    - **T4 Code Generation**
      - T4 text template files can be often identified by the .tt extension.
      - T4 is the templating and code generation engine that Entity Framework uses to generate code so you don't have to manage it yourself. 
      - These are generated based from the conceptual model in the .edmx file.
  - **Entity Data Model Designer**
    - When you have EDMX Designer open, View -> Other Windows -> Entity Data Model Browser. This is useful as your .edmx begins to cover a large number of tables and you have difficulty locating a particular one. 
    - Inheritance and Complex type options can be done in the .edmx file. 
    - **Table per Hierarchy (TPH)**
      - Is the default mapping strategy.
      - It creates a single table for all objects in an inheritance hierarchy, and it simply has nullable columns for fields that aren't common across all types. It also adds a Disciminator column so EF can keep track of the type of each individual record.  - This strategy is often the best balance of tradeoffs available because it provides the best performance. 
        - The primary disadvantage is that your data is slightly denormalized.
    - **Table per Type (TPT)**
      - Alternative strategy
      - Creates a table for your base type that has all common fields in it, a table for each child type that stores the additional fields, as well as an ID to the base type's record that stores the common fields. The multiple inheriting types' tables are linked to one another via a foreign key that has a shared primary key value. In this case, EF has to perform a join across multiple tables. 
        - The primary disadvantage is that your performance suffers.
    - **Table per Concrete Type (TPC) and Mixed Inheritance**
      - These are the other options. These are not usually the most practical choice because of the limited tooling support for them.
      - **Currently not supported in the EDM Designer, although the EF runtime does support them**
    - **Complex Type**
      - A complex type is a logical designation for a common group of fields on multiple entities. Used for reusability, as it allows you to use repeated groups of fields (e.g. a date range). 
    - **Stored Procedure Mapping**
      - based on the STored Procedures defined in the database. It enables you to specify an Update, and Delete function. SElect queries are already handled by the langauge semantics of LINQ.
  - **ObjectContext vs DbContext** 
    - Olders versions of EF did not have DbContext but had ObjectContext.
    - Modern versions still support the ObjectContext object, and you can even consume modern EF in much the same way as the older versions. 
    - If you want to work in an ObjectContext scenario, you have 3 primary options:
      - Legacy approach
        - Follow similar steps using VS 2008 or 2010 to create an .edmx file and its corresponding ObjectContext and entities
      - Downgrading your entities
        - Take the .edmx file that was generated, open the properties window, and set focus in the Design'ers canvas. 
        - Change the Code Generation Strategy from None to default in the Properties window.
        - Delete the two .tt files listed as children to your .edmx file in the Solution explorer window
      - Hybrid Approach
        - Get the ObjectContext (via a nonobvious approach) from the DbContext and work with the ObjectContext directly. Note that even with modern versions of EF, in some rare and advanced scenarios this is still required. 
  - **ObjectContext management**
    - Two things happen when ObjectContext constructor is called:
      - The generated context inherits several items from the ObjectContext base class, including a property known as ContextOptions and an event named OnContextCreated.
      - The ContextOptions class has five primary properties you can manipulate:
        - ** LazyLoadingEnabled (VERY IMPORTANT) **
          - If left unspecified, the default is true
          - Lazy loading enables entities to be loaded on-demand without thought by the developer. Although this can be handy, this behaviour can have very serious performance implications depending on how relationships are set up. 
          - Feature development and even app architecture might change one way or another based on the use of this feature. 
          - ** In EF, lazy loading is triggered based on a Navigation Property being accessed. By simply referencing a navigation property, that entity is then loaded **
            - Accessing a navigation property causes another roundtrip to the database to fetch that data. 
            - If you loop through a collection of entities, an individual roundtrip is made to the database for each entity. 
          - If you have LazyLoadedEnabled to false, you have two options:
            - Can either use explicit lazy loading with the ObjectContext's LoadProperty() method or you can use eager loading with the ObjectSet's Include() method.  
            - With eager loading, you must specify up front what related data you want loaded.
            - Explicit lazy loading, like regular lazy loading, can reduce the amount of data flowing back and forth, it's a chatty pattern, whereas eager loading is a chunky pattern. 
            - **Chatty vs Chunky Pattern**
              - Chatty is taking into consideration the latency or delay of making a roundtrip. It is doing parts at a time to get to the whole. i.e. Making multiple calls to the DB to sum the result or something.
              - Chunky pattern is doing it all at once type. i.e. Making 1 call to the database. 
              - The best option depends on different factors such as the transfer time and the delay with the roundtrips. Typically favour CHUNKY over chatty!
        - ** ProxyCreationEnabled **
          - Determines whether proxy objects should be created for custom data classes that are persistence-ignorant, such as a plan old common object (POCO) entities. 
          - Default is set to true. (Generally isn't something you need to be concerned about if you set to false).
          - Only available in .NET Framework 4.5.
        - ** UseConsistentNullReferenceBehaviour **
          - When set to false, you can attempt to set navigation property to null and nothing will happen when you try and save it. 
          - When set to true, Allows you to set navigation property to null (which may throw error if the field is not nullable)
          - LOOK THIS UP. 
        - ** UseCSharpNullComparisonBehaviour **
          - Main implication of this property is that it changes queries that require null comparisons.
        - ** UseLegacyPreserverChangesBehaviour **
  - **ObjectContext entities**
    - Each entity you define int he conceptual model gets a class generated.
    - There are few options for the types of entities that can be generated.
      - In EF 4, the default base class for entities was EntityObject.
      - Two other types of entities could also be created: POCO entities (no required base class for these) and Self-Tracking entities (STEs) (no base class, but they implemented IObjectWithChangeTracker and INotifyPropertyChanged). 
    - Attributes (In the generated classes): 
      - **EdmEntityTypeAttribute**
        - Contains the item's namespace and the Name.
        - Does little more than tell the world that the class is an EDM type and then provides a basic amount of supporting information about it. 
      - **SerializableAttribute**
        - The entity class needs to be serializable, if you want to persist the object locally, pass it between tiers, or do much else with it. 
      - **DataContractAttribute**
        - Must decorate the class (required for WCF's default serialization)
      - Look at the definition of each class that's generated corresponding to each entity and as much as possible in the .edmx. 
        - Each property needs to be marked with the DataMember attribute
        - The EdmScalarProperty attribute decorates each property definition
        - The two main properties that it definares are EntityKeyProperty and IsNullable.
  - **Why choose the Entity Framework (EF)**
    - Microsoft has made a tremendous investment in the EF and continues to, which is indicative of its future plans and where it intends data access to go. 
    - The toolset has matured, the following are the benefits:
      - There is a tremendous tooling support that enables you to build and maintain the data access elements of your application in a much shorter time than ADO.NET would
      - You can focus on dealing with business objects of your application and don't have to spend your time voerly concerned about the underlying data store. 
      - It allows for a clear separation of concerns between the conceptual model and the underlying data store. 
      - The data store can be changed (quite dramatically) without having to rewrite core application logic. Microsoft has made a tremendous investment into the EF, and all indiciations point to the fact it has made a firm commitment to the success of the EF.
      - Built to fit right in with WCF, which makes them complementary technology.
- **WCF Data Services**
  - **Choosing WCF Data Services as the data access technology**
    - Highly notable changes :
      - Open Data Protocol (OData)
      - REST
      - JSON
    - WCF Data Services, originally called ADO.NET Data services, is the main mechanism of Microsoft to deal with exposing these features to developers. 
    - WCF data services are well-suited to applications that are exposed via Service-Oriented Architecture (SOA). 
    - Web Services and Windows Communication Foundation (WCF) were big steps forward, but WCF Data Services takes it all one step farther in making the whole process a much easier endeavor.
  - **OData**
    - Abbreviated form of the "Open Data Protocol"
    - Web protocol for querying and updating data. 
    - Does this by applying and building upon Web Technologies such as HTTP, Atom Publishing Protocol (AtomPub) and JSON to provide access to information from a variety of applications, services and stores. 
    - OData is being used to expose and access information from a variety of sources (relational databases, file systems, CMS's and traditional websites etc)
    - Is consistent with the way the Web works - makes a deep commitment to URIs for resource identification and commits to an HTTP-based, uniform interface for interacting with those resources (just like the web). 
    - This commitment to core Web principles allows OData to enable a new level of data integration and interoperability across a broad range of clients, services, and tools.
  - **WCF Data Services as data access mechanisms** 
    - Because MS moved its Web Services implementation (.asmx) to WCF, service names now end in the .svc extension. 
    - Name was originally ADO.NET Data Services and has since changed WCF Data Services. 
    - **Build a WCF Service**
      - Create a ASP.NET web app
      - Use the EF to define an EDM
      - Add a Data Service to the Web Application (which is there just to host the application and expose it over the web)
      - Enable access to the service.
  - **Why choose WCF Data Services**
    - Uses EF as a foundation, most (but not all) scenarios that were appropriate for one would be appropriate for another
    - WCF Data Services would be overkill for simple 1-user scenarios. On the other hand, it provides several out-of-the-box benefits that ADO.NET and the EF do not. These include the following:
      - When using OData, resources are addressable via URIs. That makes it very nonproperitary and opens the service up to be consumed by everyone
      - WCF Data Services are accessed over HTTP, and almost everyone is familiar with HTTP theese days and has access to it. 
      - OData feeds can take several different forms including Atom, JSON, or XML. All thsoe formats can be represented as text, so many problems with firewalls, security, installing applications and so forth immediately disappear.
      - Very powerful queries can be constructed with simple semantics. Do not need to know SQL. (Can do it via URLs)
      - To take full advantage of WCF Data Services, the EF is meant to be used. But the EF can get its data from just about anywhere, and the tooling support makes it very easy to swap out or modify underlying data sources.
- **Summary of Choosing Data Access Technology**
  - ADO.NET has been around the longest and has several advantages. 
    - Does not require persistent connections to the underlying data store
    - Enables you to access virtually all major database implementations
    - Enables you to access data through custom objects (DataSet and DataTable)
  - By using Entity Framework, developers can focus on the conceptual model (solving business logic) without being overly concerned with the underlying data store. EF is specifically focused on working with an entity, but not quite as much as working with bulk data all at once
  - With EF, the underlying data stores can be easily modified and changed without requiring changes of the client code. 
  - WCF Data Services let your applications provide universal data access. The consumption of WCF Data Services is not ited to any proprietary technology, so can be consumed by both Microsoft and non-Microsoft technologies. WCF Data Services are meant to be used in conjunction with the EF on the backend. They provide a very fast and easy way to build apps and make the data easily accessible to any consumer.
- **Caching**
  - **Windows Azure-provided Caching Options**
    - **Shared or Co-Located Caching**
      - Windows Azure takes a sliver of your Web Role or Worker Role instance's resources (memory being one of the most important resources) and uses that to store the cached data. 
      - Don't have to pay anything extra in terms of price: just a slight performance hit. 
      - If your cache needs to store only a few MB of data, this is a very good option.
    - **Dedicated Caching**
      - You add a new Cache Worker Role role to your project, which results in an instance whose entire purpose in life is to manage and host the cache. 
      - Benefits is that you can easily manage the number of these instances independent of other instances, and hosting very large amounts of cahced data doesn't directly afect performance on your other servers or your app.
  - **Using the ObjectCache**
    - Use the MemoryCache Default property to get an instance of ObjectCache (since constructor is protected)
    - ObjectCache class alone provides nearly all functionality for caching
    - As noted, the items in the cache are stored as key/value pairs. The three primary properties in the CacheItem base class are, key, value and regionName. 
    - The cache implementation si allowed to store the cached data in any internal format it wants. The only restrictions are that the APIs provide that the data is represented in CacheItem objects, not that those CacheItem objects are actually stored internally. 
  - **CacheItemPolicy**
    - Once you instantiate an instance of it, you'll then want to set an expiration policy by using either an AbsoluteExpiration or a SlidingExpiration. 
    - AbsoluteExpiration
      - The CacheItem is purged after a specified amount of time
    - SlidingExpiration
      - The CacheItem is purged only if it has not been accessed after a specified amount of time. 
  - **CacheItemPriority**
    - Only 2 options in the System.Runtime.Caching namespace: Default (no priority for removing this entry), and Not Removable (never be removed from the cache)
    - More options in the System.Web.Caching namespace: Low, BelowNormal, Normal, AboveNormal, High, NotRemoveable, Default.
  - **Using the HttpContext.Cache**
    - In ASP.NET you can use Application State, Session State, and View State to store values and thereby minimize roundtrips to the database. In a typical ASP.NET web application or service that's hosted in ASp.NET, the cache is accessed through the HttpContext.Current object. 
      - The Cache class was built specifically for use with ASP.NET applications
      - If you need caching functionality in another type of application (for instance, a windows forms app), you should specifically use the ObjectCache class instead. 
      - This class is only created once per AppDomain. Once created, it remains alive (even if it's empty) as long as the AppDomain is still active. 
      - Cache items are implemented as name/value pairs where the name is implemented as string and value is object. You can put any serializable object in the cache. 
    - Abbrieviated System.Web.Caching.Cache 
    - System.Web.Caching.Cache.Cache is the implementation of .NET caching.
    - HttpContext.Cache is the instance of that implementation, that lives in the application domain.
  - **Cache Best Practice**
    - Production code should minimize the use of quoted strings, particularly in cases where the string values are being used as keys to collections (such as caches). There are several reasons for this including performance, maintainability, and ease of use.
    - Defining the string literals using the const keyword at class level is often appropriate. (It is often advisable to create strings as constants or properties and keep them in classes of their own)
  - **SqlCacheDependency**
    - Using the CacheDependency class enables you to monitor and respond to a wide variety of changes to underlying objects.
    - If you are caching data sourced from a SQL Server database (versions 7.0, 2000, 2005+ are supported), you can use the SqlCacheDependency class. 
   	- SqlCacheDependency monitors the underlying table the data orginally came from. If any changes are made to the table, the items added to the Cache are removed, and a new version of the item is added to the cache.
   	  - To create a SQLCacheDependency, you can use either of two overloads:
   	    - Provide a SqlCommand instance. This initializes a new instance of the SqlCacheDependency class and, coupled with the command, creates a cache-key dependency.
   	    - You provide two strings. The first string is the name of the database defined in the databases element of the application's web config file. The second string is the name of the table that the SqlCacheDependency will be associated with.
   	- Creating a dependency carries overhead and is laden with nuances you need to be careful about.
   	  - It's worth noting that the baggage that comes with using the SqlCacheDependency is so significant that many consider it too substantial to justify the benefits.
   	  - **LOOK AT CHAPTER IF YOU ARE INTERESTED IN USING**
- **Summary of Caching**
  - Caching data isn't a magic wand to fix performance problems, most applications have very obvious aspects that lend themselves well to data caching.
  - ObjectCache is the primary mechanism you can use to cache data
  - The Cache property of HttpContext can be used to provide caching functionality in ASP.NET applications
  - When using ObjectCache, the two most high profile elements are ExpirationPolicy and ChangeMonitoring
  - A specific date and time can trigger cache expiration, which is known as AbsoluteExpiration. For instance, by using AbsoluteExpiration, you can wipe out the cache or remove an item from it at midnight everyday, once an hour, or whatever makes sense.
  - Expiration can be handled so that an object is removed from the cache only if it has not been accessed for a certain specified interval using SlidingExpiration. For instance, you can remove an item from the cache if it has not been accessed for two hours.
